# FastAPI Image Chat API

This repository provides a FastAPI API for image-based question answering using the `MiniCPM-Llama3-V-2_5` model. The model can process images and generate responses based on the provided questions.

## Features

- Upload images and ask questions about the content.
- Supports Nvidia GPUs and Mac with MPS (Apple Silicon or AMD GPUs).
- Provides responses generated by a pre-trained model.

## Requirements

- Python 3.8 or higher
- `torch` (with CUDA support if using Nvidia GPU)
- `transformers`
- `fastapi`
- `uvicorn`
- `Pillow`

## Setup

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/your-repo-name.git
cd your-repo-name

# Create a virtual environment
python -m venv mincpm-llama3-v25-venv 

# Activate the virtual environment
# On Windows
mincpm-llama3-v25-venv\Scripts\activate
# On macOS/Linux
source mincpm-llama3-v25-venv/bin/activate
```

3. Install Dependencies
Install the required packages using pip.
```bash
pip install -r requirements.txt
```

# Running the API

### For Nvidia GPUs

Ensure you have CUDA installed and available. Run the following command:

```bash
python -m uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```
For Mac with MPS (Apple Silicon or AMD GPUs)
Enable MPS support and run the API using:

PYTORCH_ENABLE_MPS_FALLBACK=1 python -m uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload -- --device mps



```bash
curl -X 'POST' \
  'http://127.0.0.1:8000/predict/' \
  -H 'accept: application/json' \
  -H 'Content-Type: multipart/form-data' \
  -F 'file=@./assets/hk_OCR.jpg' \
  -F 'question=Where is this photo taken?'
```